{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "#from tqdm import tqdm_notebook as tqdm\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "Let $U(\\mathbf{z}) = \\frac{1}{2}\\left(\\frac{\\|\\mathbf{z}\\|-2}{0.4}\\right)^2 - \\log\\left(e^{-0.5\\left[\\frac{\\mathbf{z}_1 - 2}{0.6}\\right]} + e^{-0.5\\left[\\frac{\\mathbf{z}_1 + 2}{0.6}\\right]}\\right)$, and $p(\\mathbf{z}) \\propto e^{-U(\\mathbf{z})}$ be the distribution we want to sample from.\n",
    "\n",
    "Let's visualize the (unnormalized) density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def npdensity1(z):\n",
    "    z = np.reshape(z, [z.shape[0], 2])\n",
    "    z1, z2 = z[:, 0], z[:, 1]\n",
    "    norm = np.sqrt(z1 ** 2 + z2 ** 2)\n",
    "    exp1 = np.exp(-0.5 * ((z1 - 2) / 0.6) ** 2)\n",
    "    exp2 = np.exp(-0.5 * ((z1 + 2) / 0.6) ** 2)\n",
    "    u = 0.5 * ((norm - 2) / 0.4) ** 2 - np.log(exp1 + exp2)\n",
    "    return np.exp(-u)\n",
    "\n",
    "def npdensity2(z):\n",
    "    z = z.reshape(-1, 2)\n",
    "    x, y = z[:, 0], z[:, 1]\n",
    "    u = 0.8 * x ** 2 + (y - ((x**2)**(1/3)))**2\n",
    "    u = u / 2**2\n",
    "    return np.exp(-u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.linspace(-5, 5, 1000)\n",
    "x, y = np.meshgrid(r, r)\n",
    "z = np.vstack([x.flatten(), y.flatten()]).T\n",
    "\n",
    "q0 = npdensity1(z)\n",
    "plt.pcolormesh(x, y, q0.reshape(x.shape),\n",
    "                           cmap='viridis')\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.xlim([-3, 3])\n",
    "plt.ylim([-3, 3])\n",
    "plt.title('Density #1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.linspace(-5, 5, 1000)\n",
    "x, y = np.meshgrid(r, r)\n",
    "z = np.vstack([x.flatten(), y.flatten()]).T\n",
    "\n",
    "q0 = npdensity2(z)\n",
    "plt.pcolormesh(x, y, q0.reshape(x.shape),\n",
    "                           cmap='viridis')\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.xlim([-3.5, 3.5])\n",
    "plt.ylim([-3.5, 3.5])\n",
    "plt.title('Density #2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unadjusted Langevin Algorithm (ULA)\n",
    "\n",
    "The Langevin SDE can be simulated using the Euler-Maruyama scheme as follows\n",
    "\n",
    "$$X_{k+1} = X_{k} - \\gamma_{k+1}\\nabla U(X_k) + \\sqrt{2\\gamma_{k+1}}Z_{k+1}$$\n",
    "\n",
    "where $\\gamma_{k+1}$ is the step size and $Z_{k+1}$ is a sample from $\\mathcal{N}(0, I)$ respectively at the $k+1$-th time-step.\n",
    "\n",
    "In practice, there are various tricks to set $\\gamma_{k+1}$ and $X_0$, the initial seed. However, in the following examples, I've used a constant step-size and have sampled $X_0$ from $\\mathcal{N}(0, I)$.\n",
    "\n",
    "The above simulation of Langevin dynamics can be used to draw samples from densities of the form $p(x) = \\frac{e^{-U(x)}}{Z}$ where Z may or may not be known. It is assumed that the gradient of $U$ is $L$-Lipschtiz. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def potential1(z):\n",
    "    z = z.view(-1, 2)\n",
    "    z1, z2 = z[:, 0], z[:, 1]\n",
    "    norm = torch.norm(z, p=2, dim=1)\n",
    "    exp1 = torch.exp(-0.5 * ((z1 - 2) / 0.6) ** 2)\n",
    "    exp2 = torch.exp(-0.5 * ((z1 + 2) / 0.6) ** 2)\n",
    "    u = 0.5 * ((norm - 2) / 0.4) ** 2 - torch.log(exp1 + exp2)\n",
    "    return u\n",
    "\n",
    "def potential2(z):\n",
    "    z = z.view(-1, 2)\n",
    "    x, y = z[:, 0], z[:, 1]\n",
    "    u = 0.8 * x ** 2 + (y - ((x**2)**(1/3)))**2\n",
    "    u = u / 2**2\n",
    "    return u\n",
    "\n",
    "def torch_tunable_sigmoid(x,epsilon):\n",
    "    return 1/(1+torch.exp(- epsilon * x))\n",
    "\n",
    "def potential_ball(z,epsilon=5,r=2):\n",
    "    norm=torch.norm(z,p=2,dim=1)\n",
    "    p=torch.sigmoid((r-norm)*epsilon)\n",
    "    return -torch.log(p)\n",
    "\n",
    "def potential_square(z,epsilon=5,r=2):\n",
    "    norm=torch.norm(z,p=torch.inf,dim=1)\n",
    "    p=torch.sigmoid((r-norm)*epsilon)\n",
    "    return -torch.log(p)\n",
    "\n",
    "def potential_vote(z,epsilon=5,r=2):\n",
    "    z = z.view(-1, 2)\n",
    "    g_1=z[:,1]-z[:,0]\n",
    "    g_2=z[:,1]+z[:,0]\n",
    "    #g_3=2-z[:,1]\n",
    "    S_1=torch.sigmoid(epsilon*g_1)+torch.sigmoid(epsilon*g_2)\n",
    "    V_1=torch.sigmoid(epsilon*(S_1-0.5))\n",
    "    vote_term=torch.sigmoid(epsilon*(V_1-0.5))\n",
    "\n",
    "    norm=torch.norm(z,p=torch.inf,dim=1)\n",
    "    init_term=torch.sigmoid((r-norm)*epsilon)\n",
    "\n",
    "    full_term=vote_term*init_term\n",
    "\n",
    "    return -torch.log(full_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unadjusted_langevin_algorithm(potential, n_samples=100000, step=0.1):\n",
    "    burn_in = 10000\n",
    "    Z0 = torch.randn(1, 2)\n",
    "    Zi = Z0\n",
    "    samples = []\n",
    "    for i in tqdm(range(n_samples + burn_in)):\n",
    "        Zi.requires_grad_()\n",
    "        u = potential(Zi).mean()\n",
    "        grad = torch.autograd.grad(u, Zi)[0]\n",
    "        Zi = Zi.detach() - step * grad + np.sqrt(2 * step) * torch.randn(1, 2)\n",
    "        samples.append(Zi.detach().numpy())\n",
    "    return np.concatenate(samples, 0)[burn_in:]\n",
    "\n",
    "def unadjusted_langevin_algorithm_gpu(potential, n_samples=100000, step=0.01, iters=10000):\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"No GPU\")\n",
    "        return\n",
    "    \n",
    "    device=torch.device('cuda')\n",
    "    Z0 = torch.randn(n_samples, 2, device=device)\n",
    "    Zi = Z0\n",
    "    for i in tqdm(range(iters)):\n",
    "        Zi.requires_grad_()\n",
    "        u = potential(Zi).sum()\n",
    "        grad = torch.autograd.grad(u, Zi)[0]\n",
    "        #print(grad)\n",
    "        Zi = Zi.detach() - step * grad + np.sqrt(2 * step) * torch.randn(n_samples, 2, device=device)\n",
    "        #samples.append(Zi.detach().cpu().numpy())\n",
    "    return Zi.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the ULA and render the empirical density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples1 = unadjusted_langevin_algorithm(potential1)\n",
    "samples2 = unadjusted_langevin_algorithm(potential2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_gpu = unadjusted_langevin_algorithm_gpu(potential1,step=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_gpu_2_list=[]\n",
    "for i in range(10):\n",
    "    samples_gpu_2_list.append(unadjusted_langevin_algorithm_gpu(potential2,n_samples=1000,step=0.01))\n",
    "\n",
    "samples_gpu_2 = np.concatenate(samples_gpu_2_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_gpu_ball = unadjusted_langevin_algorithm_gpu(lambda z: potential_ball(z,5,1.5),n_samples=100000,step=0.1,iters=20000)\n",
    "samples_gpu_sq = unadjusted_langevin_algorithm_gpu(lambda z: potential_square(z,5,1.5),n_samples=100000,step=0.1,iters=20000)\n",
    "samples_gpu_vote = unadjusted_langevin_algorithm_gpu(lambda z: potential_vote(z,5,2),n_samples=100000,step=0.001,iters=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_gpu_vote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_gpu_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist2d(samples1[:,0], samples1[:,1], cmap='viridis', rasterized=False, bins=200, density=True)\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.xlim([-3, 3])\n",
    "plt.ylim([-3, 3])\n",
    "plt.title('Empirical Density #1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist2d(samples_gpu[:,0], samples_gpu[:,1], cmap='viridis', rasterized=False, bins=200, density=True)\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.xlim([-3, 3])\n",
    "plt.ylim([-3, 3])\n",
    "plt.title('Empirical Density #1 GPU')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist2d(samples2[:,0], samples2[:,1], cmap='viridis', rasterized=False, bins=200, density=True)\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.xlim([-3.5, 3.5])\n",
    "plt.ylim([-3.5, 3.5])\n",
    "plt.title('Empirical Density #2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist2d(samples_gpu_2[:,0], samples_gpu_2[:,1], cmap='viridis', rasterized=False, bins=200, density=True)\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.xlim([-3.5, 3.5])\n",
    "plt.ylim([-3.5, 3.5])\n",
    "plt.title('Empirical Density #2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist2d(samples_gpu_ball[:,0], samples_gpu_ball[:,1], cmap='viridis', rasterized=False, bins=200, density=True)\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.xlim([-3.5, 3.5])\n",
    "plt.ylim([-3.5, 3.5])\n",
    "plt.title('Empirical Density Ball GPU')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist2d(samples_gpu_sq[:,0], samples_gpu_sq[:,1], cmap='viridis', rasterized=False, bins=200, density=True)\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.xlim([-3.5, 3.5])\n",
    "plt.ylim([-3.5, 3.5])\n",
    "plt.title('Empirical Density Square GPU')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist2d(samples_gpu_vote[:,0], samples_gpu_vote[:,1], cmap='viridis', rasterized=False, bins=200, density=True)\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.xlim([-3.5, 3.5])\n",
    "plt.ylim([-3.5, 3.5])\n",
    "plt.title('Empirical Density Square GPU')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metropolis-adjusted Langevin Algorithm (MALA)\n",
    "\n",
    "New samples are proposed using the Euler-Maruyama scheme as before, but are accepted/rejected using the Metropolis-Hastings algorithm, i.e., the acceptance propability is given by\n",
    "\n",
    "$$\\alpha = \\min \\left\\{1, \\frac{p(x_{k+1})Q(x_k|x_{k+1})}{p(x_{k})Q(x_{k+1}|x_{k})}\\right\\}$$\n",
    "\n",
    "where \n",
    "\n",
    "$$Q(x'|x) \\propto \\exp\\left(-\\frac{1}{4\\gamma}\\|x' - x + \\gamma\\nabla U(x)\\|^2\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_Q(potential, z_prime, z, step):\n",
    "    z.requires_grad_()\n",
    "    grad = torch.autograd.grad(potential(z).mean(), z)[0]\n",
    "    return -(torch.norm(z_prime - z + step * grad, p=2, dim=1) ** 2) / (4 * step)\n",
    "\n",
    "def log_Q_gpu(potential, z_prime, z, step):\n",
    "    z.requires_grad_()\n",
    "    grad = torch.autograd.grad(potential(z).sum(), z)[0]\n",
    "    return -(torch.norm(z_prime - z + step * grad, p=2, dim=1) ** 2) / (4 * step)\n",
    "\n",
    "def metropolis_adjusted_langevin_algorithm(potential, n_samples=100000, step=0.1):\n",
    "    burn_in = 10000\n",
    "    Z0 = torch.randn(1, 2)\n",
    "    Zi = Z0\n",
    "    samples = []\n",
    "    pbar = tqdm(range(n_samples + burn_in))\n",
    "    for i in pbar:\n",
    "        Zi.requires_grad_()\n",
    "        u = potential(Zi).mean()\n",
    "        grad = torch.autograd.grad(u, Zi)[0]\n",
    "        prop_Zi = Zi.detach() - step * grad + np.sqrt(2 * step) * torch.randn(1, 2)\n",
    "        log_ratio = -potential(prop_Zi).mean() + potential(Zi).mean() +\\\n",
    "                    log_Q(potential, Zi, prop_Zi, step) - log_Q(potential, prop_Zi, Zi, step)\n",
    "        if torch.rand(1) < torch.exp(log_ratio):\n",
    "            Zi = prop_Zi\n",
    "        samples.append(Zi.detach().numpy())\n",
    "    return np.concatenate(samples, 0)[burn_in:]\n",
    "\n",
    "def metropolis_adjusted_langevin_algorithm_gpu(potential, n_samples=100000, step=0.1, iters=10000):\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"No GPU\")\n",
    "        return\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    device=torch.device('cuda')\n",
    "    Z0 = torch.randn(n_samples, 2, device=device)\n",
    "    Zi = Z0\n",
    "    for i in tqdm(range(iters)):\n",
    "        #torch.cuda.empty_cache()\n",
    "        Zi.requires_grad_()\n",
    "        u = potential(Zi).sum()\n",
    "        grad = torch.autograd.grad(u, Zi)[0]\n",
    "        #print(grad)\n",
    "        prop_Zi = Zi.detach() - step * grad + np.sqrt(2 * step) * torch.randn(n_samples, 2, device=device)\n",
    "        log_ratio = -potential(prop_Zi) + potential(Zi) +\\\n",
    "                    log_Q_gpu(potential, Zi, prop_Zi, step) - log_Q_gpu(potential, prop_Zi, Zi, step)\n",
    "        mask= torch.rand(n_samples,device=device) < torch.exp(log_ratio)\n",
    "        #print(mask)\n",
    "        mask=mask.double().view(n_samples,-1)\n",
    "        Zi=mask*prop_Zi + (1-mask)*Zi.detach()\n",
    "        #samples.append(Zi.detach().cpu().numpy())\n",
    "    return Zi.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples1 = metropolis_adjusted_langevin_algorithm(potential1)\n",
    "samples2 = metropolis_adjusted_langevin_algorithm(potential2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample1_adj_gpu = metropolis_adjusted_langevin_algorithm_gpu(potential1,n_samples=100000,iters=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample1_adj_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample2_adj_gpu = metropolis_adjusted_langevin_algorithm_gpu(potential2,n_samples=100000,iters=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample2_adj_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_adj_gpu_ball = metropolis_adjusted_langevin_algorithm_gpu(lambda z: potential_ball(z,5,1.5),n_samples=100000,step=0.01,iters=1000)\n",
    "samples_adj_gpu_sq = metropolis_adjusted_langevin_algorithm_gpu(lambda z: potential_square(z,5,1.5),n_samples=100000,step=0.01,iters=1000)\n",
    "samples_adj_gpu_vote = metropolis_adjusted_langevin_algorithm_gpu(lambda z: potential_vote(z,5,2),n_samples=100000,step=0.01,iters=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_adj_gpu_vote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist2d(samples1[:,0], samples1[:,1], cmap='viridis', rasterized=False, bins=200, density=True)\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.xlim([-3, 3])\n",
    "plt.ylim([-3, 3])\n",
    "plt.title('Empirical Density #1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist2d(sample1_adj_gpu[:,0], sample1_adj_gpu[:,1], cmap='viridis', rasterized=False, bins=200, density=True)\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.xlim([-3, 3])\n",
    "plt.ylim([-3, 3])\n",
    "plt.title('Empirical Density #1 GPU')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist2d(samples2[:,0], samples2[:,1], cmap='viridis', rasterized=False, bins=200, density=True)\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.xlim([-3.5, 3.5])\n",
    "plt.ylim([-3.5, 3.5])\n",
    "plt.title('Empirical Density #2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist2d(sample2_adj_gpu[:,0], sample2_adj_gpu[:,1], cmap='viridis', rasterized=False, bins=200, density=True)\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.xlim([-3.5, 3.5])\n",
    "plt.ylim([-3.5, 3.5])\n",
    "plt.title('Empirical Density #2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist2d(samples_adj_gpu_ball[:,0], samples_adj_gpu_ball[:,1], cmap='viridis', rasterized=False, bins=200, density=True)\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.xlim([-3.5, 3.5])\n",
    "plt.ylim([-3.5, 3.5])\n",
    "plt.title('Empirical Density Ball GPU')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist2d(samples_adj_gpu_sq[:,0], samples_adj_gpu_sq[:,1], cmap='viridis', rasterized=False, bins=200, density=True)\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.xlim([-3.5, 3.5])\n",
    "plt.ylim([-3.5, 3.5])\n",
    "plt.title('Empirical Density Square GPU')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist2d(samples_adj_gpu_vote[:,0], samples_adj_gpu_vote[:,1], cmap='viridis', rasterized=False, bins=200, density=True)\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.xlim([-3.5, 3.5])\n",
    "plt.ylim([-3.5, 3.5])\n",
    "plt.title('Empirical Density Square GPU')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metropolis-Hastings Algorithm for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metropolis_hastings(target_density, size=100000):\n",
    "    burnin_size = 10000\n",
    "    size += burnin_size\n",
    "    x0 = np.array([[0, 0]])\n",
    "    xt = x0\n",
    "    samples = []\n",
    "    for i in tqdm(range(size)):\n",
    "        xt_candidate = np.array([np.random.multivariate_normal(xt[0], np.eye(2))])\n",
    "        accept_prob = (target_density(xt_candidate))/(target_density(xt))\n",
    "        if np.random.uniform(0, 1) < accept_prob:\n",
    "            xt = xt_candidate\n",
    "        samples.append(xt)\n",
    "    samples = np.array(samples[burnin_size:])\n",
    "    samples = np.reshape(samples, [samples.shape[0], 2])\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples1 = metropolis_hastings(npdensity1)\n",
    "samples2 = metropolis_hastings(npdensity2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist2d(samples1[:,0], samples1[:,1], cmap='viridis', rasterized=False, bins=200, density=True)\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.xlim([-3, 3])\n",
    "plt.ylim([-3, 3])\n",
    "plt.title('Empirical Density #1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist2d(samples2[:,0], samples2[:,1], cmap='viridis', rasterized=False, bins=200, density=True)\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.xlim([-3.5, 3.5])\n",
    "plt.ylim([-3.5, 3.5])\n",
    "plt.title('Empirical Density #2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
